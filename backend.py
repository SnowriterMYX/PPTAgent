import asyncio
import hashlib
import importlib
import json
import os
import signal
import sys
import threading
import time
import traceback
import uuid

import chardet
import docx
from contextlib import asynccontextmanager
from copy import deepcopy
from datetime import datetime
from pathlib import Path
from typing import Optional

from dotenv import load_dotenv
from fastapi import (
    FastAPI,
    File,
    Form,
    HTTPException,
    Request,
    UploadFile,
    WebSocket,
    WebSocketDisconnect,
)
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse

import pptagent.induct as induct
import pptagent.pptgen as pptgen
from pptagent.document import Document
from pptagent.llms import llm_logger
from pptagent.model_utils import ModelManager, parse_pdf
from pptagent.multimodal import ImageLabler
from pptagent.presentation import Presentation
from pptagent.utils import Config, get_logger, package_join, pjoin, ppt_to_images_async

# åŠ è½½ç¯å¢ƒå˜é‡
# é¦–å…ˆå°è¯•ä»å½“å‰ç›®å½•åŠ è½½ .env æ–‡ä»¶
env_paths = [
    Path.cwd() / ".env",  # å½“å‰å·¥ä½œç›®å½•
    Path(__file__).parent.parent / ".env",  # é¡¹ç›®æ ¹ç›®å½•
    Path(__file__).parent / ".env",  # UIç›®å½•
]

for env_path in env_paths:
    if env_path.exists():
        load_dotenv(env_path)
        print(f"âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡æ–‡ä»¶: {env_path}")
        break
else:
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°è¯•åŠ è½½é»˜è®¤ä½ç½®
    load_dotenv()
    print("âš ï¸  æœªæ‰¾åˆ° .env æ–‡ä»¶ï¼Œä½¿ç”¨ç³»ç»Ÿç¯å¢ƒå˜é‡")

# constants
DEBUG = os.environ.get("DEBUG", "true").lower() == "true" if len(sys.argv) == 1 else False
RUNS_DIR = package_join("runs")
STAGES = [
    "PPT Parsing",
    "PDF Parsing",
    "PPT Analysis",
    "PPT Generation",
    "Success!",
]

# åˆå§‹åŒ–æ¨¡å‹ç®¡ç†å™¨
models = ModelManager()

# å…¨å±€å˜é‡ç”¨äºä¼˜é›…å…³é—­
shutdown_event = asyncio.Event()
_shutdown_in_progress = False

def signal_handler(signum, frame):
    """å¤„ç† Ctrl+C ä¿¡å·"""
    global _shutdown_in_progress
    if _shutdown_in_progress:
        print(f"\nâš ï¸  å·²åœ¨å…³é—­ä¸­ï¼Œè¯·ç¨ç­‰...")
        return

    _shutdown_in_progress = True
    print(f"\nğŸ›‘ æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œæ­£åœ¨ä¼˜é›…å…³é—­æœåŠ¡...")
    shutdown_event.set()

    # å¦‚æœåœ¨ä¸»çº¿ç¨‹ä¸­ï¼Œå¯ä»¥ç›´æ¥é€€å‡º
    if threading.current_thread() is threading.main_thread():
        # ç»™ä¸€äº›æ—¶é—´è®©æ¸…ç†å®Œæˆ
        time.sleep(2)
        print("ğŸ”„ å¼ºåˆ¶é€€å‡º...")
        os._exit(0)

# æ³¨å†Œä¿¡å·å¤„ç†å™¨
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

@asynccontextmanager
async def lifespan(_: FastAPI):
    # æµ‹è¯•æ¨¡å‹è¿æ¥ï¼Œä½†ä¸å¼ºåˆ¶è¦æ±‚æˆåŠŸï¼ˆå¼€å‘æ¨¡å¼ï¼‰
    print("ğŸš€ å¯åŠ¨PPTAgentåç«¯æœåŠ¡...")

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs(RUNS_DIR, exist_ok=True)
    os.makedirs(pjoin(RUNS_DIR, "feedback"), exist_ok=True)
    print("ğŸ“ å·²åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„")

    connection_ok = await models.test_connections()
    if connection_ok:
        print("âœ… æ‰€æœ‰æ¨¡å‹è¿æ¥æµ‹è¯•é€šè¿‡")
    else:
        print("âš ï¸  æ¨¡å‹è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œä½†ç»§ç»­å¯åŠ¨ï¼ˆå¼€å‘æ¨¡å¼ï¼‰")
        print("ğŸ“ è¯·æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„APIé…ç½®")
        print("ğŸ’¡ æ‚¨å¯ä»¥ç¨ååœ¨ç•Œé¢ä¸­æµ‹è¯•æ¨¡å‹è¿æ¥")

    yield

    # ä¼˜é›…å…³é—­å¤„ç†
    print("ğŸ”„ æ­£åœ¨æ¸…ç†èµ„æº...")
    try:
        # è®¾ç½®æ¸…ç†è¶…æ—¶æ—¶é—´
        cleanup_timeout = 10  # 10ç§’è¶…æ—¶

        # æ¸…ç†æ¨¡å‹èµ„æºï¼ˆå¸¦è¶…æ—¶ï¼‰
        if hasattr(models, 'cleanup'):
            try:
                await asyncio.wait_for(models.cleanup(), timeout=cleanup_timeout)
            except asyncio.TimeoutError:
                logger.warning("æ¨¡å‹æ¸…ç†è¶…æ—¶ï¼Œå¼ºåˆ¶ç»§ç»­")
            except Exception as e:
                logger.error(f"æ¨¡å‹æ¸…ç†å‡ºé”™: {e}")

        # ç­‰å¾…æ‰€æœ‰æ´»è·ƒè¿æ¥å…³é—­ï¼ˆå¸¦è¶…æ—¶ï¼‰
        if active_connections:
            print(f"â³ ç­‰å¾… {len(active_connections)} ä¸ªæ´»è·ƒè¿æ¥å…³é—­...")
            close_tasks = []
            for task_id, websocket in list(active_connections.items()):
                if websocket:
                    close_tasks.append(websocket.close())

            if close_tasks:
                try:
                    await asyncio.wait_for(
                        asyncio.gather(*close_tasks, return_exceptions=True),
                        timeout=5
                    )
                except asyncio.TimeoutError:
                    logger.warning("WebSocketè¿æ¥å…³é—­è¶…æ—¶")

            active_connections.clear()

        print("âœ… èµ„æºæ¸…ç†å®Œæˆ")
    except Exception as e:
        logger.error(f"æ¸…ç†èµ„æºæ—¶å‡ºé”™: {e}")
    finally:
        print("ğŸ‘‹ PPTAgentåç«¯æœåŠ¡å·²åœæ­¢")


# server
logger = get_logger(__name__)
app = FastAPI(lifespan=lifespan)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
progress_store: dict[str, dict] = {}
active_connections: dict[str, WebSocket] = {}


class ProgressManager:
    def __init__(self, task_id: str, stages: list[str], debug: bool = True):
        self.task_id = task_id
        self.stages = stages
        self.debug = debug
        self.task_id = task_id
        self.failed = False
        self.current_stage = 0
        self.total_stages = len(stages)

    async def report_progress(self):
        assert (
            self.task_id in active_connections
        ), "WebSocket connection is already closed"
        self.current_stage += 1
        progress = int((self.current_stage / self.total_stages) * 100)
        await send_progress(
            active_connections[self.task_id],
            f"Stage: {self.stages[self.current_stage - 1]}",
            progress,
        )

    async def fail_stage(self, error_message: str):
        await send_progress(
            active_connections[self.task_id],
            f"{self.stages[self.current_stage]} Error: {error_message}",
            100,
        )
        self.failed = True
        active_connections.pop(self.task_id, None)
        if self.debug:
            logger.error(
                f"{self.task_id}: {self.stages[self.current_stage]} Error: {error_message}"
            )


@app.post("/api/upload")
async def create_task(
    pptxFile: UploadFile = File(None),
    pdfFile: UploadFile = File(None),
    textFile: UploadFile = File(None),  # æ–°å¢ï¼šæ”¯æŒæ–‡æœ¬æ–‡ä»¶
    topic: str = Form(...),  # æ”¹ä¸ºå¿…å¡«
    userInput: str = Form(None),  # æ–°å¢ï¼šæ”¯æŒç”¨æˆ·ç›´æ¥è¾“å…¥
    numberOfPages: int = Form(...),
    # æ–°å¢ï¼šä¸»é¢˜é…ç½®å‚æ•°
    targetAudience: str = Form(None),
    presentationStyle: str = Form(None),
    userContext: str = Form(None),
    generateTopicContent: bool = Form(True),
):
    task_id = datetime.now().strftime("20%y-%m-%d") + "/" + str(uuid.uuid4())
    logger.info(f"task created: {task_id}")
    os.makedirs(pjoin(RUNS_DIR, task_id))
    task = {
        "numberOfPages": numberOfPages,
        "pptx": "default_template",
    }
    if pptxFile is not None:
        pptx_blob = await pptxFile.read()
        pptx_md5 = hashlib.md5(pptx_blob).hexdigest()
        task["pptx"] = pptx_md5
        pptx_dir = pjoin(RUNS_DIR, "pptx", pptx_md5)
        if not os.path.exists(pptx_dir):
            os.makedirs(pptx_dir, exist_ok=True)
            with open(pjoin(pptx_dir, "source.pptx"), "wb") as f:
                f.write(pptx_blob)
    if pdfFile is not None:
        pdf_blob = await pdfFile.read()
        pdf_md5 = hashlib.md5(pdf_blob).hexdigest()
        task["pdf"] = pdf_md5
        pdf_dir = pjoin(RUNS_DIR, "pdf", pdf_md5)
        if not os.path.exists(pdf_dir):
            os.makedirs(pdf_dir, exist_ok=True)
            with open(pjoin(pdf_dir, "source.pdf"), "wb") as f:
                f.write(pdf_blob)

    # å¤„ç†æ–‡æœ¬æ–‡ä»¶ä¸Šä¼ 
    if textFile is not None:
        text_blob = await textFile.read()
        text_md5 = hashlib.md5(text_blob).hexdigest()
        task["textFile"] = text_md5
        task["textFileName"] = textFile.filename
        text_dir = pjoin(RUNS_DIR, "text", text_md5)
        if not os.path.exists(text_dir):
            os.makedirs(text_dir, exist_ok=True)
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            file_ext = os.path.splitext(textFile.filename)[1].lower()
            source_filename = f"source{file_ext}"
            with open(pjoin(text_dir, source_filename), "wb") as f:
                f.write(text_blob)

    # å¤„ç†ç”¨æˆ·ç›´æ¥è¾“å…¥
    if userInput is not None and userInput.strip():
        input_md5 = hashlib.md5(userInput.encode('utf-8')).hexdigest()
        task["userInput"] = input_md5
        input_dir = pjoin(RUNS_DIR, "input", input_md5)
        if not os.path.exists(input_dir):
            os.makedirs(input_dir, exist_ok=True)
            # ä¿å­˜ç”¨æˆ·è¾“å…¥ä¸ºæ–‡æœ¬æ–‡ä»¶
            with open(pjoin(input_dir, "user_input.txt"), "w", encoding="utf-8") as f:
                f.write(userInput)

    # ä¿å­˜ä¸»é¢˜é…ç½®
    task["topic"] = topic
    if targetAudience:
        task["targetAudience"] = targetAudience
    if presentationStyle:
        task["presentationStyle"] = presentationStyle
    if userContext:
        task["userContext"] = userContext
    task["generateTopicContent"] = generateTopicContent

    progress_store[task_id] = task
    # Start the PPT generation task asynchronously
    asyncio.create_task(ppt_gen(task_id))
    return {"task_id": task_id.replace("/", "|")}


async def send_progress(websocket: Optional[WebSocket], status: str, progress: int):
    if websocket is None:
        logger.info(f"websocket is None, status: {status}, progress: {progress}")
        return
    try:
        await websocket.send_json({"progress": progress, "status": status})
    except Exception as e:
        logger.warning(f"Failed to send progress: {e}")
        # WebSocketå·²æ–­å¼€ï¼Œä¸éœ€è¦æŠ›å‡ºå¼‚å¸¸


@app.websocket("/wsapi/{task_id}")
async def websocket_endpoint(websocket: WebSocket, task_id: str):
    original_task_id = task_id
    task_id = task_id.replace("|", "/")
    logger.info(f"WebSocket connection attempt: original={original_task_id}, decoded={task_id}")
    logger.info(f"Available tasks in progress_store: {list(progress_store.keys())}")

    if task_id in progress_store:
        await websocket.accept()
        active_connections[task_id] = websocket
        logger.info(f"WebSocket connected for task: {task_id}")
        try:
            # ä¿æŒè¿æ¥æ´»è·ƒï¼Œç­‰å¾…å®¢æˆ·ç«¯æ–­å¼€æˆ–ä»»åŠ¡å®Œæˆ
            while task_id in active_connections:
                # ç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯æˆ–æ–­å¼€è¿æ¥
                try:
                    message = await asyncio.wait_for(websocket.receive_text(), timeout=1.0)
                    logger.debug(f"Received message from client: {message}")
                except asyncio.TimeoutError:
                    # è¶…æ—¶æ˜¯æ­£å¸¸çš„ï¼Œç»§ç»­å¾ªç¯
                    continue
        except WebSocketDisconnect:
            logger.info("websocket disconnected: %s", task_id)
            active_connections.pop(task_id, None)
        except Exception as e:
            logger.error(f"WebSocket error for task {task_id}: {e}")
            active_connections.pop(task_id, None)
    else:
        # å¯¹äºWebSocketï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ¥å—è¿æ¥ç„¶åå…³é—­
        await websocket.accept()
        await websocket.close(code=1008, reason="Task not found")
        logger.warning("WebSocket connection rejected: task %s not found in %s", task_id, list(progress_store.keys()))


@app.get("/api/download")
async def download(task_id: str):
    task_id = task_id.replace("|", "/")
    if not os.path.exists(pjoin(RUNS_DIR, task_id)):
        raise HTTPException(status_code=404, detail="Task not created yet")
    file_path = pjoin(RUNS_DIR, task_id, "final.pptx")
    if os.path.exists(file_path):
        return FileResponse(
            file_path,
            media_type="application/pptx",
            headers={"Content-Disposition": "attachment; filename=pptagent.pptx"},
        )
    raise HTTPException(status_code=404, detail="Task not finished yet")


@app.post("/api/feedback")
async def feedback(request: Request):
    body = await request.json()
    feedback_text = body.get("feedback")
    task_id = body.get("task_id")

    if not feedback_text or not task_id:
        raise HTTPException(status_code=400, detail="Feedback and task_id are required")

    # åˆ›å»ºfeedbackç›®å½•
    feedback_dir = pjoin(RUNS_DIR, "feedback")
    os.makedirs(feedback_dir, exist_ok=True)

    # æ¸…ç†task_idä¸­çš„éæ³•å­—ç¬¦ï¼ˆWindowsæ–‡ä»¶åä¸èƒ½åŒ…å« | ç­‰å­—ç¬¦ï¼‰
    safe_task_id = task_id.replace("|", "_").replace(":", "_").replace("/", "_").replace("\\", "_")

    # æ·»åŠ æ—¶é—´æˆ³å’Œç¼–ç æ”¯æŒ
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{safe_task_id}_{timestamp}.txt"

    try:
        with open(pjoin(feedback_dir, filename), "w", encoding="utf-8") as f:
            f.write(f"Task ID: {task_id}\n")
            f.write(f"Timestamp: {datetime.now().isoformat()}\n")
            f.write(f"Feedback:\n{feedback_text}\n")

        logger.info(f"Feedback saved: {filename}")
        return {"message": "Feedback submitted successfully", "filename": filename}

    except Exception as e:
        logger.error(f"Failed to save feedback: {e}")
        raise HTTPException(status_code=500, detail="Failed to save feedback")


@app.get("/")
async def hello():
    return {"message": "Hello, World!"}

@app.get("/api/")
async def api_hello():
    return {"message": "Hello, World!"}


@app.get("/api/llm-logs/{task_id}")
async def get_llm_logs(task_id: str):
    """
    è·å–æŒ‡å®šä»»åŠ¡çš„LLMè¯·æ±‚è®°å½•

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        LLMè¯·æ±‚è®°å½•åˆ—è¡¨
    """
    try:
        # è§£ç ä»»åŠ¡ID
        decoded_task_id = task_id.replace("|", "/")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pjoin(RUNS_DIR, decoded_task_id)):
            raise HTTPException(status_code=404, detail="Task not found")

        # è·å–LLMè®°å½•
        logs = llm_logger.get_logs(decoded_task_id)

        return {
            "task_id": task_id,
            "logs": logs,
            "total_count": len(logs)
        }

    except Exception as e:
        logger.error(f"Failed to get LLM logs for task {task_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve LLM logs")


@app.get("/api/llm-logs/{task_id}/summary")
async def get_llm_logs_summary(task_id: str):
    """
    è·å–æŒ‡å®šä»»åŠ¡çš„LLMè¯·æ±‚è®°å½•æ‘˜è¦

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        LLMè¯·æ±‚è®°å½•æ‘˜è¦ç»Ÿè®¡
    """
    try:
        # è§£ç ä»»åŠ¡ID
        decoded_task_id = task_id.replace("|", "/")

        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(pjoin(RUNS_DIR, decoded_task_id)):
            raise HTTPException(status_code=404, detail="Task not found")

        # è·å–LLMè®°å½•
        logs = llm_logger.get_logs(decoded_task_id)

        # ç»Ÿè®¡ä¿¡æ¯
        summary = {
            "total_requests": len(logs),
            "successful_requests": len([log for log in logs if log.get("status") == "success"]),
            "failed_requests": len([log for log in logs if log.get("status") == "error"]),
            "stages": {},
            "model_types": {},
            "total_duration_ms": 0,
            "total_tokens": 0
        }

        for log in logs:
            # æŒ‰é˜¶æ®µç»Ÿè®¡
            stage = log.get("stage", "unknown")
            if stage not in summary["stages"]:
                summary["stages"][stage] = 0
            summary["stages"][stage] += 1

            # æŒ‰æ¨¡å‹ç±»å‹ç»Ÿè®¡
            model_type = log.get("model_type", "unknown")
            if model_type not in summary["model_types"]:
                summary["model_types"][model_type] = 0
            summary["model_types"][model_type] += 1

            # ç´¯è®¡è€—æ—¶
            summary["total_duration_ms"] += log.get("duration_ms", 0)

            # ç´¯è®¡tokenä½¿ç”¨
            response = log.get("response", {})
            tokens = response.get("tokens_used", 0)
            if tokens:
                summary["total_tokens"] += tokens

        return {
            "task_id": task_id,
            "summary": summary
        }

    except Exception as e:
        logger.error(f"Failed to get LLM logs summary for task {task_id}: {e}")
        raise HTTPException(status_code=500, detail="Failed to retrieve LLM logs summary")


async def ppt_gen(task_id: str, rerun=False):
    if DEBUG:
        importlib.reload(induct)
        importlib.reload(pptgen)
    if rerun:
        task_id = task_id.replace("|", "/")
        active_connections[task_id] = None
        with open(pjoin(RUNS_DIR, task_id, "task.json"), "r", encoding="utf-8") as f:
            progress_store[task_id] = json.load(f)

    # Wait for WebSocket connection
    for _ in range(100):
        if task_id in active_connections:
            break
        await asyncio.sleep(0.02)
    else:
        progress_store.pop(task_id)
        return

    task = progress_store.pop(task_id)
    pptx_md5 = task["pptx"]
    generation_config = Config(pjoin(RUNS_DIR, task_id))
    pptx_config = Config(pjoin(RUNS_DIR, "pptx", pptx_md5))
    json.dump(task, open(pjoin(generation_config.RUN_DIR, "task.json"), "w", encoding="utf-8"), ensure_ascii=False)
    progress = ProgressManager(task_id, STAGES)

    # æ£€æŸ¥æ–‡æ¡£æºç±»å‹
    has_pdf = "pdf" in task and task["pdf"] is not None
    has_text_file = "textFile" in task and task["textFile"] is not None
    has_user_input = "userInput" in task and task["userInput"] is not None
    has_topic = "topic" in task and task["topic"] is not None

    # ç¡®å®šæ–‡æ¡£æºç›®å½•
    if has_pdf:
        pdf_md5 = task["pdf"]
        parsedpdf_dir = pjoin(RUNS_DIR, "pdf", pdf_md5)
    elif has_text_file:
        text_md5 = task["textFile"]
        parsedpdf_dir = pjoin(RUNS_DIR, "text", text_md5)
    elif has_user_input:
        input_md5 = task["userInput"]
        parsedpdf_dir = pjoin(RUNS_DIR, "input", input_md5)
    elif has_topic:
        # ä¸ºtopicåˆ›å»ºä¸€ä¸ªå”¯ä¸€çš„ç›®å½•
        topic_hash = hashlib.md5(task["topic"].encode('utf-8')).hexdigest()
        parsedpdf_dir = pjoin(RUNS_DIR, "topic", topic_hash)
        os.makedirs(parsedpdf_dir, exist_ok=True)
    else:
        await progress.fail_stage("No document source provided (PDF, text file, user input, or topic)")
        return
    ppt_image_folder = pjoin(pptx_config.RUN_DIR, "slide_images")

    await send_progress(
        active_connections[task_id], "task initialized successfully", 10
    )

    try:
        # è®¾ç½®LLMè®°å½•å™¨ä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "ppt_parsing")

        # ppt parsing
        presentation = Presentation.from_file(
            pjoin(pptx_config.RUN_DIR, "source.pptx"), pptx_config
        )
        if not os.path.exists(ppt_image_folder) or len(
            os.listdir(ppt_image_folder)
        ) != len(presentation):
            await ppt_to_images_async(
                pjoin(pptx_config.RUN_DIR, "source.pptx"), ppt_image_folder
            )
            assert len(os.listdir(ppt_image_folder)) == len(presentation) + len(
                presentation.error_history
            ), "Number of parsed slides and images do not match"

            for err_idx, _ in presentation.error_history:
                os.remove(pjoin(ppt_image_folder, f"slide_{err_idx:04d}.jpg"))
            for i, slide in enumerate(presentation.slides, 1):
                slide.slide_idx = i
                os.rename(
                    pjoin(ppt_image_folder, f"slide_{slide.real_idx:04d}.jpg"),
                    pjoin(ppt_image_folder, f"slide_{slide.slide_idx:04d}.jpg"),
                )

        # è®¾ç½®å›¾åƒæ ‡æ³¨é˜¶æ®µä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "image_caption")

        labler = ImageLabler(presentation, pptx_config)
        if os.path.exists(pjoin(pptx_config.RUN_DIR, "image_stats.json")):
            image_stats = json.load(
                open(pjoin(pptx_config.RUN_DIR, "image_stats.json"), encoding="utf-8")
            )
            labler.apply_stats(image_stats)
        else:
            await labler.caption_images_async(models.vision_model)
            json.dump(
                labler.image_stats,
                open(
                    pjoin(pptx_config.RUN_DIR, "image_stats.json"),
                    "w",
                    encoding="utf-8",
                ),
                ensure_ascii=False,
                indent=4,
            )
        await progress.report_progress()

        # è®¾ç½®PDFè§£æé˜¶æ®µä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "pdf_parsing")

        # æ–‡æ¡£è§£æå¤„ç†
        source_contents = []  # ç”¨äºå­˜å‚¨å¤šä¸ªå†…å®¹æº

        if not os.path.exists(pjoin(parsedpdf_dir, "source.md")):
            # å¤„ç†å¤šç§å†…å®¹æº

            # 1. å¤„ç†ä¸»é¢˜å†…å®¹ç”Ÿæˆ
            if has_topic and task.get("generateTopicContent", True):
                # å‘é€è‡ªå®šä¹‰è¿›åº¦æ¶ˆæ¯
                await send_progress(
                    active_connections[task_id],
                    "æ­£åœ¨ç”Ÿæˆä¸»é¢˜ç›¸å…³å†…å®¹...",
                    30
                )

                try:
                    # åˆ›å»ºä¸»é¢˜å†…å®¹ç”Ÿæˆagent
                    from pptagent.agent import AsyncAgent
                    topic_generator = AsyncAgent(
                        "topic_content_generator",
                        llm_mapping={"language": models.language_model}
                    )

                    # ç”Ÿæˆè¯¦ç»†å†…å®¹
                    generated_content = await topic_generator(
                        topic=task["topic"],
                        user_context=task.get("userContext", ""),
                        target_audience=task.get("targetAudience", ""),
                        presentation_style=task.get("presentationStyle", "")
                    )

                    # ç¡®ä¿ç”Ÿæˆçš„å†…å®¹æ˜¯å­—ç¬¦ä¸²
                    if isinstance(generated_content, tuple):
                        # å¦‚æœè¿”å›çš„æ˜¯å…ƒç»„ï¼Œå–ç¬¬äºŒä¸ªå…ƒç´ ï¼ˆé€šå¸¸æ˜¯å®é™…å†…å®¹ï¼‰
                        generated_content = generated_content[1] if len(generated_content) > 1 else str(generated_content[0])
                    elif not isinstance(generated_content, str):
                        generated_content = str(generated_content)

                    source_contents.append(("ä¸»é¢˜ç”Ÿæˆå†…å®¹", generated_content))

                except Exception as e:
                    logger.warning(f"ä¸»é¢˜å†…å®¹ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡æ¿: {e}")
                    # å¦‚æœAIç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ¨¡æ¿
                    basic_content = f"""# {task['topic']}

## æ¦‚è¿°
è¿™æ˜¯å…³äº"{task['topic']}"çš„æ¼”ç¤ºæ–‡ç¨¿å†…å®¹ã€‚

## ä¸»è¦å†…å®¹
è¯·æ ¹æ®ä»¥ä¸‹è¦ç‚¹å±•å¼€ï¼š
- èƒŒæ™¯ä»‹ç»
- æ ¸å¿ƒè¦ç‚¹
- å…·ä½“æ¡ˆä¾‹
- æ€»ç»“å±•æœ›

## è¡¥å……ä¿¡æ¯
{task.get('userContext', 'æš‚æ— è¡¥å……ä¿¡æ¯')}

## ç›®æ ‡å—ä¼—
{task.get('targetAudience', 'é€šç”¨å—ä¼—')}

## æ¼”ç¤ºé£æ ¼
{task.get('presentationStyle', 'æ ‡å‡†æ¼”ç¤º')}"""

                    source_contents.append(("ä¸»é¢˜ç”Ÿæˆå†…å®¹", basic_content))

            # 2. å¤„ç†æ–‡æ¡£æ–‡ä»¶
            if has_pdf:
                # è§£æPDFæ–‡ä»¶
                pdf_content = parse_pdf(
                    pjoin(RUNS_DIR, "pdf", pdf_md5, "source.pdf"),
                    parsedpdf_dir,
                    models.marker_model,
                )
                source_contents.append(("PDFæ–‡æ¡£", pdf_content))
            elif has_text_file:
                # è§£ææ–‡æœ¬æ–‡ä»¶
                text_md5 = task["textFile"]
                text_dir = pjoin(RUNS_DIR, "text", text_md5)

                # æŸ¥æ‰¾æºæ–‡ä»¶
                source_files = [f for f in os.listdir(text_dir) if f.startswith("source")]
                if not source_files:
                    await progress.fail_stage("Text source file not found")
                    return

                source_file = pjoin(text_dir, source_files[0])
                file_type = os.path.splitext(source_files[0])[1].lower()

                # æ ¹æ®æ–‡ä»¶ç±»å‹è§£æ
                if file_type == '.md' or file_type == '.markdown':
                    with open(source_file, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                elif file_type == '.txt':
                    # æ£€æµ‹ç¼–ç å¹¶è¯»å–
                    with open(source_file, 'rb') as f:
                        raw_data = f.read()
                        encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'
                    with open(source_file, 'r', encoding=encoding) as f:
                        content = f.read()
                    # ç®€å•æ ¼å¼åŒ–ä¸ºmarkdown
                    file_content = f"# {task.get('textFileName', 'æ–‡æ¡£')}\n\n{content}"
                elif file_type == '.docx':
                    # è§£æDOCXæ–‡ä»¶
                    doc = docx.Document(source_file)
                    content_parts = [f"# {task.get('textFileName', 'æ–‡æ¡£')}", ""]

                    for paragraph in doc.paragraphs:
                        text = paragraph.text.strip()
                        if text:
                            if paragraph.style.name.startswith('Heading'):
                                level = paragraph.style.name.replace('Heading ', '')
                                if level.isdigit():
                                    content_parts.append(f"{'#' * (int(level) + 1)} {text}")
                                else:
                                    content_parts.append(f"## {text}")
                            else:
                                content_parts.append(text)
                            content_parts.append("")

                    file_content = "\n".join(content_parts)
                else:
                    # å…¶ä»–æ ¼å¼ï¼Œå°è¯•ä½œä¸ºçº¯æ–‡æœ¬å¤„ç†
                    with open(source_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    file_content = f"# {task.get('textFileName', 'æ–‡æ¡£')}\n\n{content}"

                source_contents.append(("æ–‡æœ¬æ–‡ä»¶", file_content))

            # 3. å¤„ç†ç”¨æˆ·è¾“å…¥
            if has_user_input:
                # å¤„ç†ç”¨æˆ·ç›´æ¥è¾“å…¥
                input_md5 = task["userInput"]
                input_dir = pjoin(RUNS_DIR, "input", input_md5)

                with open(pjoin(input_dir, "user_input.txt"), "r", encoding="utf-8") as f:
                    user_content = f.read()

                # æ ¼å¼åŒ–ç”¨æˆ·è¾“å…¥ä¸ºmarkdown
                lines = user_content.strip().split('\n')
                formatted_lines = ["# ç”¨æˆ·è¾“å…¥æ–‡æ¡£", ""]

                current_section = []
                for line in lines:
                    line = line.strip()
                    if not line:
                        if current_section:
                            formatted_lines.extend(current_section)
                            formatted_lines.append("")
                            current_section = []
                        continue

                    # æ£€æµ‹æ˜¯å¦å¯èƒ½æ˜¯æ ‡é¢˜
                    if (len(line) < 50 and
                        not line.endswith('.') and
                        not line.endswith(',') and
                        not line.endswith(';')):
                        if current_section:
                            formatted_lines.extend(current_section)
                            formatted_lines.append("")
                            current_section = []
                        formatted_lines.append(f"## {line}")
                        formatted_lines.append("")
                    else:
                        current_section.append(line)

                if current_section:
                    formatted_lines.extend(current_section)

                user_input_content = "\n".join(formatted_lines)
                source_contents.append(("ç”¨æˆ·è¾“å…¥", user_input_content))

            # 4. åˆå¹¶æ‰€æœ‰å†…å®¹æº
            if source_contents:
                # åˆå¹¶å¤šä¸ªå†…å®¹æº
                merged_content_parts = [f"# {task['topic']}"]
                merged_content_parts.append("")

                for source_name, content in source_contents:
                    # ç¡®ä¿å†…å®¹æ˜¯å­—ç¬¦ä¸²ç±»å‹
                    if not isinstance(content, str):
                        logger.warning(f"Content from {source_name} is not string: {type(content)}")
                        content = str(content)

                    merged_content_parts.append(f"## {source_name}")
                    merged_content_parts.append("")
                    merged_content_parts.append(content)
                    merged_content_parts.append("")

                # ç¡®ä¿æ‰€æœ‰éƒ¨åˆ†éƒ½æ˜¯å­—ç¬¦ä¸²
                string_parts = []
                for part in merged_content_parts:
                    if isinstance(part, str):
                        string_parts.append(part)
                    else:
                        logger.warning(f"Non-string part found: {type(part)} - {part}")
                        string_parts.append(str(part))

                text_content = "\n".join(string_parts)
            else:
                # ä»…æœ‰ä¸»é¢˜ï¼Œæ— å…¶ä»–å†…å®¹æº
                text_content = f"# {task['topic']}\n\nè¯·åŸºäºè¿™ä¸ªä¸»é¢˜ç”Ÿæˆæ¼”ç¤ºæ–‡ç¨¿å†…å®¹ã€‚"

            # ä¿å­˜åˆå¹¶åçš„å†…å®¹åˆ°source.md
            with open(pjoin(parsedpdf_dir, "source.md"), "w", encoding="utf-8") as f:
                f.write(text_content)
        else:
            text_content = open(
                pjoin(parsedpdf_dir, "source.md"), encoding="utf-8"
            ).read()
        await progress.report_progress()

        # è®¾ç½®æ–‡æ¡£ä¼˜åŒ–é˜¶æ®µä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "document_refine")

        # document refine
        if not os.path.exists(pjoin(parsedpdf_dir, "refined_doc.json")):
            source_doc = await Document.from_markdown_async(
                text_content,
                models.language_model,
                models.vision_model,
                parsedpdf_dir,
            )
            json.dump(
                source_doc.to_dict(),
                open(pjoin(parsedpdf_dir, "refined_doc.json"), "w", encoding="utf-8"),
                ensure_ascii=False,
                indent=4,
            )
        else:
            try:
                with open(pjoin(parsedpdf_dir, "refined_doc.json"), "r", encoding="utf-8") as f:
                    source_doc = json.load(f)
                source_doc = Document.from_dict(source_doc, parsedpdf_dir)
            except (json.JSONDecodeError, UnicodeDecodeError) as e:
                logger.error(f"Failed to load refined_doc.json: {e}")
                # å¦‚æœJSONæ–‡ä»¶æŸåï¼Œé‡æ–°ç”Ÿæˆ
                logger.info("Regenerating document due to corrupted JSON file...")
                source_doc = await Document.from_markdown_async(
                    text_content,
                    models.language_model,
                    models.vision_model,
                    parsedpdf_dir,
                )
                json.dump(
                    source_doc.to_dict(),
                    open(pjoin(parsedpdf_dir, "refined_doc.json"), "w", encoding="utf-8"),
                    ensure_ascii=False,
                    indent=4,
                )
        await progress.report_progress()

        # è®¾ç½®å¹»ç¯ç‰‡å½’çº³é˜¶æ®µä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "slide_induction")

        # Slide Induction
        if not os.path.exists(pjoin(pptx_config.RUN_DIR, "slide_induction.json")):
            deepcopy(presentation).save(
                pjoin(pptx_config.RUN_DIR, "template.pptx"), layout_only=True
            )
            await ppt_to_images_async(
                pjoin(pptx_config.RUN_DIR, "template.pptx"),
                pjoin(pptx_config.RUN_DIR, "template_images"),
            )
            slide_inducter = induct.SlideInducterAsync(
                presentation,
                ppt_image_folder,
                pjoin(pptx_config.RUN_DIR, "template_images"),
                pptx_config,
                models.image_model,
                models.language_model,
                models.vision_model,
            )
            layout_induction = await slide_inducter.layout_induct()
            slide_induction = await slide_inducter.content_induct(layout_induction)
            json.dump(
                slide_induction,
                open(
                    pjoin(pptx_config.RUN_DIR, "slide_induction.json"),
                    "w",
                    encoding="utf-8",
                ),
                ensure_ascii=False,
                indent=4,
            )
        else:
            slide_induction = json.load(
                open(
                    pjoin(pptx_config.RUN_DIR, "slide_induction.json"), encoding="utf-8"
                )
            )
        await progress.report_progress()

        # è®¾ç½®PPTç”Ÿæˆé˜¶æ®µä¸Šä¸‹æ–‡
        llm_logger.set_context(task_id, "ppt_generation")

        # PPT Generation with PPTAgentAsync
        ppt_agent = pptgen.PPTAgentAsync(
            models.text_model,
            models.language_model,
            models.vision_model,
            error_exit=False,
            retry_times=5,
            sim_bound=0.2,  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ä»¥å¤„ç†ç« èŠ‚åŒ¹é…é—®é¢˜
        )
        ppt_agent.set_reference(
            config=generation_config,
            slide_induction=slide_induction,
            presentation=presentation,
        )

        prs, _ = await ppt_agent.generate_pres(
            source_doc=source_doc,
            num_slides=task["numberOfPages"],
        )
        prs.save(pjoin(generation_config.RUN_DIR, "final.pptx"))
        logger.info(f"{task_id}: generation finished")
        await progress.report_progress()
    except Exception as e:
        await progress.fail_stage(str(e))
        traceback.print_exc()


if __name__ == "__main__":
    import uvicorn

    async def run_server():
        """å¼‚æ­¥è¿è¡ŒæœåŠ¡å™¨"""
        config = uvicorn.Config(
            app,
            host="0.0.0.0",
            port=9297,
            log_level="info",
            access_log=True,
            # æ·»åŠ ä¼˜é›…å…³é—­é…ç½®
            timeout_keep_alive=5,
            timeout_graceful_shutdown=10,
        )
        server = uvicorn.Server(config)

        # ç›‘å¬å…³é—­äº‹ä»¶
        async def shutdown_monitor():
            await shutdown_event.wait()
            print("ï¿½ å¼€å§‹å…³é—­æœåŠ¡å™¨...")
            server.should_exit = True

        # å¯åŠ¨å…³é—­ç›‘å¬å™¨
        shutdown_task = asyncio.create_task(shutdown_monitor())

        try:
            await server.serve()
        finally:
            shutdown_task.cancel()
            try:
                await shutdown_task
            except asyncio.CancelledError:
                pass

    try:
        ip = "0.0.0.0"
        print("ğŸš€ å¯åŠ¨PPTAgentåç«¯æœåŠ¡...")
        print(f"ğŸŒ æœåŠ¡åœ°å€: http://{ip}:9297")
        print("ğŸ“ ä½¿ç”¨ Ctrl+C åœæ­¢æœåŠ¡")
        print("=" * 50)

        # è¿è¡Œå¼‚æ­¥æœåŠ¡å™¨
        asyncio.run(run_server())

    except KeyboardInterrupt:
        print("\nğŸ›‘ æ¥æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨åœæ­¢æœåŠ¡...")
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å¯åŠ¨å¤±è´¥: {e}")
        traceback.print_exc()
    finally:
        print("ğŸ‘‹ PPTAgentåç«¯æœåŠ¡å·²åœæ­¢")
